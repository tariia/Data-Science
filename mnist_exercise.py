# -*- coding: utf-8 -*-
"""
Created on Mon Oct 11 22:03:04 2021

@author: romal
"""

# =============================================================================
# Каждая цифра представлена в виде черно-белой картинки (оттенок представлен числами от 0 до 255).
# Число входов сети будет равно общему количеству пикселей в картинке (например, 50*50=250 входных узлов).
# Входные узлы это один большой столбец, составленный из столбцов пикселей картинки. Т. е. первый столбец, ниже второй и т.д.
# Сеть будет использовать softmax как функцию активации, следовательно, на выходе будет вектор вероятностей (показывает насколько сильно цифра на картинке похожа на одну из 10 цифр).
# Для данной сети достаточно создать два скрытых слоя.
# =============================================================================
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

# Преобразовываем значения цветов из шкалы 0-255 в 0-1
def scale(image, lable):
    image = tf.cast(image, tf.float32)
    image /= 255.
    return image, lable

def main():
    # загружаем набор данных = 70 000 (встроен в tensorflow + там встроено еще много разных наборов; погуглить)
    # as_supervised загрузит данные в виде 2-tuple structure [input, target]. Делается для удобства
    # with_info provides a tuple containing info about version, features, number of samples of dataset 
    mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
    
    # Делим все данные на тренировочные = 60 000 (тренировочные+валидация) и тестовые = 10 000
    mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']
    
    # Здаем, сколько (именно количество, а не сами данные) даннных отделить для валидации
    # Например, это 10%. Умножаем 0.1 на кол-во данных (для простоты используем mnist_info, в которой это есть)
    num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
    # На случай, если это не целое число, исправляем, чтобы потом не было ошибок + преобразуем переменную в тензор
    num_validation_samples = tf.cast(num_validation_samples, tf.int64)
    
# =============================================================================
#     splits - это словарь
#     можно отдельно написать print(mnist_info.splits) и увидим все его содержание
#     splits['train'] - это элемент словаря; в нем содержится переменная num_examples
#     Т. е. mnist_info.splits['train'].num_examples вызывает словарь, в котором берется ключ train и вызывается значение переменной num_examples
# =============================================================================
    
    # Отдельно еще запишем, сколько тестовых переменных (не обязательно, их кол-во есть в mnist_info)
    num_test_samples = mnist_info.splits['test'].num_examples
    # Опять преобразуем в тензор и так, чтобы точно было целое число
    num_test_samples = tf.cast(num_test_samples, tf.int64)
    
    # Сохраняем в новой переменной обработанные данные (см. описание функции)
    scaled_train_and_validation_data = mnist_train.map(scale)
    test_data = mnist_test.map(scale)
    
    
# =============================================================================
#     На всякий случай для точности модели перемешиваем данные
#     Эта переменная нужна на случай, если количество данный настлько большое, что не поместится в память компьютера
#     Для этого делаем по частям
#     Если это число будет = 1, то перемешивания не будет 
#     Если больше объема данных, то перемешается только один раз
#     Если 1 < buffer_size < объем даных, то перемешается столько раз, сколько получится и оптимизируется computational power (быстрее)
# =============================================================================
    BUFFER_SIZE = 10000 # можно менять как нам надо
    
    # Перемешиваем данные пачками (по BUFFER_SIZE в каждой) 
    shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)
    
    # Создаем validation dataset (до этого было только кол-во данных, которое должно быть как валидация)
    validation_data = shuffled_train_and_validation_data.take(num_validation_samples)
    
    # И разделяем train and validation data
    train_data = shuffled_train_and_validation_data.skip(num_validation_samples)
    
# =============================================================================
#     Для оптимизации будем пользоваться градиентным спуском
#     Веса и смещение будут обновляться каждые BATCH_SIZE раз.
#     Т.е. для того, чтобы обновить w и b не надо проходить по всем данным и считать общий lossи уже тогда обновлять.
#     Тут мы проходим только по BATCH_SIZE данным, считаем loss и обновляем w и b
# =============================================================================
    BATCH_SIZE = 100 # также можем менять как нам надо
    
    # Разбиваем все данные на куски по BATCH_SIZE в каждом
    # Команда добавит еще один столбик в тензор, в котором будет написано, сколько данных брать для каждого куска
    train_data = train_data.batch(BATCH_SIZE)
    
# =============================================================================
#     Валидацию разбивать не надо, поскольку при прогоне этих данных вес и смещение изменять НЕ НАДО.
#     Эти данные нужны для отслеживания переобучения.
#     НО надо все равно перезаписать валидацию, т.к. программа ожидает ее в формате группы данных.
#     Поэтому задаем размер куска данных как размер всех валидационных данных.
# =============================================================================
    validation_data = validation_data.batch(num_validation_samples)
    
    # Аналогично для тестовых данных
    test_data = test_data.batch(num_test_samples)
    
# =============================================================================
#     В ДУШЕ НЕ ЧАЮ ЗАЧЕМ ЭТО. РАЗОБРАТЬСЯ!!!!!!
#     В лекциях объяснение, что формат должен совпадать с тестовой и тренировочной выборкой. Но он и так совпадает...
# =============================================================================
# =============================================================================
#     ОБЪЯСНЕНИЕ, но тоже не осбо понятное
#     Для того, чтобы обучить модель, надо отдельно дать модели на вход validation_inputs и validation_targets.
#     Т. е. надо обязательно их как-то разделить, чтобы было возможно обучение
#     train data и test data были созданы изначально, а не в процессе наших манипуляций, поэтому их разделять НЕ НАДО.
#     Модель сама понимает, что там inputs, а что targets.
#     Лучше объяснения пока нет.
# =============================================================================
    validation_inputs, validation_targets = next(iter(validation_data))
    
    # Создаем саму сеть
    input_size = 784
    output_size = 10
    hidden_layer_size = 50
    
    model = tf.keras.Sequential([
                                tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # переводим картинку в один столбик; в скобках указан размер картинки
                                tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #activation function also can be changed
                                tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # relu just works better for this problem
                                tf.keras.layers.Dense(output_size, activation='softmax') # выходной слой выдает вектор с вероятностями                                
                                ])
    
# =============================================================================
#     Так как у нас данные по категориям и надо, в общем-то, определить категорию, будем использовать соответствующую loss function
#     binary_crossentropy не подходит, что логично
#     categorical_crossentropy уже ожидает целевые данные в виде последовательности 0 и 1
#     sparse_categorical_crossentropy сначала переписывает данные в виде последовательности 0 и 1, а потом уже работает с ними.
#     
#     accuracy дополнительный парамерт (который может быть другим, но обычно именно этот), который будет также высчитываться
#     Тут точность нужна для отслеживания того, насколько хорошо loss сщвпадает с targets
# =============================================================================
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
    
# =============================================================================
#     Гиперпараметры (число итераций, размер буффера и прочее) лучше задавать в отдельной переменной, чтобы потом было проще искать
# =============================================================================
    NUM_EPOCHS = 5 # также можем поменять
    
    model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)

    print('done')
       
if __name__ == "__main__":
    main()